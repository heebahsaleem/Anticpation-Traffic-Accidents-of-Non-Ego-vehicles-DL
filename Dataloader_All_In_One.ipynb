{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b2a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parser\n",
    "#Parser library :\n",
    "import argparse\n",
    "import yaml \n",
    "def parse_args(a=None):\n",
    "    file_name = r\"C://Users//user//Downloads//scene_agnostic_anticipation//scene_agnostic_anticipation-main//configs//od_yolox.yaml\"\n",
    "   \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--load_config',\n",
    "                        dest='config_file',\n",
    "                        default=file_name,\n",
    "                        # type=argparse.FileType(mode='r'),\n",
    "                        help='The yaml configuration file')\n",
    "\n",
    "    if a == None:\n",
    "        args, unprocessed_args = parser.parse_known_args()\n",
    "    else:\n",
    "        args, unprocessed_args = parser.parse_known_args(args=a)\n",
    "\n",
    "    if args.config_file:\n",
    "        with open(args.config_file, 'r') as f:\n",
    "#            try:\n",
    "                parser.set_defaults(**yaml.safe_load(f))\n",
    "#            except:\n",
    "#                print('Exception_1')\n",
    "\n",
    "        try:\n",
    "                args = parser.parse_known_args(unprocessed_args)\n",
    "        except:\n",
    "                print('EXCEPTION_2')\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d497e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(FPS=10.0, H=720, W=1280, actions='FFTFF', annotation='DoTA_Non_Ego_Done_220216_relabelled.xlsx', batch_infer=30, batch_size=1, best_fol_model='weights/KEY_REP_fol_epoch_121_loss_0.0063.pt', best_harmonic_model='epoch_032.pt', box_enc_size=1024, channels=3, checkpoint='', config_file='C://Users//user//Downloads//scene_agnostic_anticipation//scene_agnostic_anticipation-main//configs//od_yolox.yaml', data_list='DoTA_split_videos.json', data_root='C:/DoTA_Avail_220726/NEgo/', dec_hidden_size=512, detect_config='./lib/mmdetection/configs/yolox/yolox_x_8x8_300e_coco.py', detect_thresh=0.5, detect_weights='./weights/yolox_x_8x8_300e_coco_20211126_140254-1ef88d67.pth', detected='Detected_yolox', device='cuda:0', dm_enc_size=1024, dota_original_annotations='/HDD/accident_anticipation/Data/DoTA_annotations/annotations', enc_concat_type='cat', enc_hidden_size=512, evaluation_save='Eval_yolox', frames='Frames', from_folder='../test_vid', future_context_size=128, gui_show=1, hiddens='Hiddens', img_format='jpg', infer_save='draw_test', input_embed_size=512, lr=0.0001, max_age=10, num_workers=1, overwrite='TRUE', pred_dim=4, pred_timesteps=10, predictor_input_size=512, repeat=1, seed_max=5, segment_len=16, shuffle=False, summary='summaries', test_anno='anno_yolox.xlsx', test_root='test', tracked='Tracked_yolox', tracked_avails='Tracked_avail', train_epoch=100, train_root='train', trial_name='weights', updated_anno='anno_yolox.xlsx', with_ego=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args()\n",
    "args = args[0]\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a7e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start\n",
    "# In this dataloader_split file, we are trying to split the data into train and test with the proportion of \n",
    "# 70% as train and 30 % as test;\n",
    "# The folder structure have a root folder and further it is divided into - Frames, Detected, Tracked, Tracked_Avails;\n",
    "# Frames -  This folder includes the raw images that needs to be passed when running the project from scratch;\n",
    "# VideoNames as Foldername -> Raw images/Frames of the videos with 10fps;\n",
    "# Detected - This folder includes the Bounding Box coordinates , x, y, cx, cy in the form of text file with the name given\n",
    "# as video_name.txt format;\n",
    "# Tracked - This folder includes the visualization folder and pickle files; visualization folder includes the frames with \n",
    "# Tracked_avails - This folder includes the pickel file only in the format - video_name.pkl\n",
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pickle5 as pickle \n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "#from IPython.core.debugger import set_trace\n",
    "\n",
    "class dataloader_split(data.Dataset):\n",
    "    \n",
    "    def __init__(self, args, foldertype, phase):       \n",
    "        self.foldertype = foldertype\n",
    "        self.args = args\n",
    "        self.phase=phase\n",
    "        self.tlst=[]\n",
    "        self.all_inputs=[]\n",
    "# Here, we are using the foldertype - [tracked, tracked_avails, detected, frames]\n",
    "\n",
    "        if self.foldertype == self.args.tracked_avails:\n",
    "# Here, we are calling the tracked_avails*. Now, the data_root* path will be created via path mentioned in .yaml file \n",
    "# Then the sessions will save the list of all the files in the recursive manner i.e., all the files stored in directories\n",
    "# subdirectories will be saved in the list named as sessions.\n",
    "# now the __tracked_avails__() here returns the dictionary that is passed in the dataframe\n",
    "\n",
    "                    if len(self.args.tracked_avails) > 0:\n",
    "                        self.data_root = os.path.join(self.args.data_root, self.args.tracked_avails)\n",
    "                        self.sessions = glob(os.path.join(self.data_root, '**/*.*'), recursive = True)\n",
    "                        self.dataframe = pd.DataFrame(self.__returndict__())\n",
    "                        self.tracked_avails_list = self.__combination__()    \n",
    "                        self.pickle_dataframe_filtered = self.creating_dataframe()\n",
    "                        \n",
    "\n",
    "                    \n",
    "        elif self.foldertype == self.args.tracked:\n",
    "# same as above    \n",
    "# This one is for the tracked folder \n",
    "\n",
    "                    if len(self.args.tracked) > 0:\n",
    "                        self.data_root = os.path.join(self.args.data_root, self.args.tracked)\n",
    "                        self.sessions = glob(os.path.join(self.data_root, '**/*.*'), recursive = True)\n",
    "                        self.dataframe = pd.DataFrame(self.__returndict__())\n",
    " \n",
    "                    \n",
    "        elif self.foldertype == self.args.frames:\n",
    "# same as above             \n",
    "# This one is for the frames folder             \n",
    "            \n",
    "                    if len(self.args.frames) > 0:\n",
    "                        self.data_root = os.path.join(self.args.data_root, self.args.frames)\n",
    "                        self.sessions = glob(os.path.join(self.data_root, '**/*.*'), recursive = True)\n",
    "                        self.dataframe = pd.DataFrame(self.__returndict__())\n",
    "\n",
    "                        \n",
    "        else:\n",
    "# same as above             \n",
    "# This one is for the detected folder             \n",
    "            \n",
    "                    if len(self.args.detected) > 0:\n",
    "                        self.data_root = os.path.join(self.args.data_root, self.args.detected)\n",
    "                        self.sessions = glob(os.path.join(self.data_root, '**/*.*'), recursive = True)\n",
    "                        self.dataframe = pd.DataFrame(self.__returndict__())\n",
    "\n",
    "                        \n",
    "        self.__split__()\n",
    "        \n",
    "        return None\n",
    "                \n",
    "\n",
    "    def __flush_dataframe__(self):\n",
    "        del self.dataframe\n",
    "        del self.train_df\n",
    "        del self.test_df\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_inputs)\n",
    "    \n",
    "# To return the values as test, train dataframe and list which is actually a combination of pickle files     \n",
    "# Here the values are collected at random basis after shuffling \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_all_dec_h, target_risk_score, except_pred = self.all_inputs[index]\n",
    "        input_all_dec_h = torch.FloatTensor(input_all_dec_h).to(self.args.device)\n",
    "        target_risk_score = torch.FloatTensor(target_risk_score).to(self.args.device)\n",
    "        #target_risk_score= target_risk_score.view(-1,1)\n",
    "        except_pred = torch.FloatTensor(except_pred).to(self.args.device)\n",
    "        #target_risk_score= torch.squeeze(target_risk_score)\n",
    "\n",
    "        return input_all_dec_h, target_risk_score, except_pred\n",
    "        \n",
    "        \n",
    "# Here all the files data are stored in the form of dictionary as per the requirement.\n",
    "# Example if there is tracked avails pickle file then its video name, file path and file type are stored in the dictionary\n",
    "# Then it is called in the init () to initialise the dataframe \n",
    "\n",
    "\n",
    "    def __returndict__(self):\n",
    "        lst = []\n",
    "        file_type = []\n",
    "        if self.foldertype == self.args.tracked_avails:\n",
    "            for sessions in self.sessions:\n",
    "                if sessions.endswith('.pkl'):\n",
    "                    file_type.append('pkl')\n",
    "                lst.append(sessions.split('\\\\')[-2])        # Video names due to the folders as stated here \n",
    "\n",
    "        elif self.foldertype == self.args.tracked:\n",
    "             for sessions in self.sessions:\n",
    "                if sessions.endswith('.jpg') or sessions.endswith('.jpeg') or sessions.endswith('.png'): \n",
    "                    file_type.append('Images')\n",
    "                    lst.append(sessions.split('\\\\')[-3])\n",
    "\n",
    "                elif sessions.endswith('.pkl'):\n",
    "                    file_type.append('pkl')\n",
    "                    lst.append(sessions.split('\\\\')[-2])       # Video names due to the folders as stated here \n",
    "                    \n",
    "        elif self.foldertype == self.args.frames:\n",
    "            for sessions in self.sessions:\n",
    "                if sessions.endswith('.jpg') or sessions.endswith('.jpeg') or sessions.endswith('.png'):\n",
    "                    file_type.append(\"image\") \n",
    "                lst.append(sessions.split('\\\\')[-2])\n",
    "        \n",
    "        else:\n",
    "            for sessions in self.sessions:\n",
    "                if sessions.endswith(\".txt\"):\n",
    "                    file_type.append('Text')\n",
    "                lst.append(sessions.split('.')[-2])\n",
    "                \n",
    "        dict = {'File_Path' : self.sessions,\n",
    "                'Video_Name': lst,\n",
    "                'File Type' : file_type\n",
    "               }\n",
    "        #print(\"return dict\")\n",
    "        return (dict)\n",
    "\n",
    "    def fiveframes(self):\n",
    "        #excel1=pd.read_excel(\"C:/Users/Heebah Saleem/Desktop/df1_common_frames.xlsx\")\n",
    "        #self.train_df= self.train_df[(self.train_df['accident_frame_common_frames']!=1000)& (self.train_df['accident_frame_common_frames']!=-1)]\n",
    "        #Col: Flag. Flag = 0 means >=5 else Flag =1 means <5\n",
    "        #1. dic: count and flag \n",
    "        #2 count: >4 flag =1 else 0\n",
    "\n",
    "        flag = []\n",
    "        count = []\n",
    "        for i in range(len(self.train_df['common_frames'])):\n",
    "                        list = str(self.train_df['common_frames'].iloc[i]).split(',')\n",
    "                        #print(list)\n",
    "                        count.append(len(list))\n",
    "                        flag.append(1 if len(list)>4 else 0)\n",
    "        #self.train_df['count']=count\n",
    "        self.train_df['flag']=flag\n",
    "        self.train_df=self.train_df.where(self.train_df['flag']==1)\n",
    "        #print(len(self.train_df))\n",
    "        self.train_df=self.train_df.dropna()\n",
    "        #print(len(self.train_df))\n",
    "        #print(\"chal gaya\")\n",
    "        return 0\n",
    "\n",
    "    def __split__(self):\n",
    "\n",
    "# This function will split the dataframe['Video_Name'] column into train, test\n",
    "# As, self.dataframe.Video_Name.unique() : represents the selecting the unique values from a column in the dataframe \n",
    "# Reason : As dataframe is a form of table therefore, the mapping will be 1 to many. Thus, at column Video_Name if there \n",
    "# is a video name as abc then it will be repeating multiple times with respect to the fellow file associated to it in the \n",
    "# file_name column\n",
    "# Therefore, when we do the train_test_split, it will fail to split the videonames by 70 : 30 percentage uniquely due to \n",
    "# the redundancy in the column. \n",
    "# Here, train and test stores the list of videonames in the 70:30 proportion\n",
    "        if args.tracked_avails:\n",
    "                train, test = train_test_split(self.pickle_dataframe_filtered.video_names.unique(), test_size = 0.3)\n",
    "                if self.phase == 'Train':\n",
    "                    self.train_df = self.pickle_dataframe_filtered.where(self.pickle_dataframe_filtered['video_names'].isin(train))\n",
    "                    self.train_df = self.train_df.dropna()\n",
    "                    #print(len(self.train_df))\n",
    "                    #print(type(self.train_df['accident_frame_common_frames'].iloc[0]))\n",
    "                    \n",
    "                    \n",
    "                    #self.train_df.to_excel(r'C:\\DoTA_Avail_220726\\self_train_df.xlsx', index = False)\n",
    "                    for i in range(len(self.train_df['accident_frame_common_frames'])):\n",
    "                            k = self.train_df['accident_frame_common_frames'].iloc[i]\n",
    "                            t=str(k).split('.')[0]\n",
    "                            t=t.replace(\"[\",\"\")\n",
    "                            self.train_df['accident_frame_common_frames'].iloc[i]= int(t)\n",
    "                    \n",
    "                    #Before used: 620positive and 700 negative data\n",
    "                    #Current: 432 postive and 432 negative\n",
    "                    positive_df=self.train_df[(self.train_df['accident_frame_common_frames']!=1000)& (self.train_df['accident_frame_common_frames']!=-1)]\n",
    "                    #positive_df.to_excel(r'C:\\DoTA_Avail_220726\\positive_train_df.xlsx', index = False)\n",
    "                    self.fiveframes()\n",
    "                    negative_df=self.train_df[(self.train_df['accident_frame_common_frames']==1000) | (self.train_df['accident_frame_common_frames']==-1)].sample(432)\n",
    "                    negative_df.groupby('video_names').nth(0)\n",
    "                    #negative_df.to_excel(r'C:\\DoTA_Avail_220726\\negative_train_df.xlsx', index = False)\n",
    "                    #then append\n",
    "                    print(\"positive df\",len(positive_df))\n",
    "                    print(\"negative df\",len(negative_df))\n",
    "                    self.train_df = positive_df.append(negative_df)\n",
    "                    #print(\"Before 5 frames\", len(self.train_df))\n",
    "                    #self.fiveframes()\n",
    "                    #print(\"After 5 frames\", len(self.train_df))\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                    self.train_df = self.train_df.sample(frac=1)\n",
    "                    #print(len(self.train_df))\n",
    "                    for i in range(0,len(self.train_df)):\n",
    "                        z, target_risk_score= self.extraction(self.train_df['values_names'].iloc[i][0], self.train_df['values_names'].iloc[i][1], self.train_df['common_frames'].iloc[i], self.train_df['video_names'].iloc[i])                      \n",
    "                        #print(type(z),type(target_risk_score))\n",
    "                        except_pred = np.ones(z.shape[0], dtype = float)\n",
    "                        self.all_inputs.append([z, target_risk_score,except_pred])\n",
    "\n",
    "            \n",
    "                    \n",
    "                else:\n",
    "                    self.test_df  = self.pickle_dataframe_filtered.where(self.pickle_dataframe_filtered['video_names'].isin(test))\n",
    "                    self.test_df  = self.test_df.dropna()\n",
    "                    \n",
    "                    #620positive and 700 negative data\n",
    "                    positive_df=self.test_df[(self.test_df['accident_frame_common_frames']!=1000)& (self.test_df['accident_frame_common_frames']!=-1)]\n",
    "                    negative_df=self.test_df[(self.test_df['accident_frame_common_frames']==1000) | (self.test_df['accident_frame_common_frames']==-1)].head(700)\n",
    "                    #then append\n",
    "                    self.test_df = positive_df.append(negative_df)\n",
    "                    \n",
    "                    self.test_df = self.test_df.sample(frac=1)\n",
    "                    for i in range(0,len(self.test_df)):\n",
    "                        z, target_risk_score= self.extraction(self.test_df['values_names'].iloc[i][0], self.test_df['values_names'].iloc[i][1], self.test_df['common_frames'].iloc[i], self.test_df['video_names'].iloc[i])\n",
    "                        except_pred = np.ones(z.shape[0], dtype = float)\n",
    "                        self.all_inputs.append([z, target_risk_score,except_pred])\n",
    "\n",
    "        else:\n",
    "            train, test = train_test_split(self.dataframe.Video_Name.unique(), test_size = 0.3)\n",
    "# here the dataframe is filtered with by comparing all the values stored in the train. Thus only those values will be kept\n",
    "# that are stored in train list and others will be dropped using .dropna()\n",
    "        \n",
    "        \n",
    "            self.train_df = self.dataframe.where(self.dataframe['Video_Name'].isin(train))\n",
    "            self.train_df = self.train_df.dropna()\n",
    "            self.test_df  = self.dataframe.where(self.dataframe['Video_Name'].isin(test))\n",
    "            self.test_df  = self.test_df.dropna()\n",
    "            \n",
    " # same is implemented for the test_df as well \n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# Here, the combination of pickle files into groups are created \n",
    "    def __combination__(self):    \n",
    "        #print(\"combination\")\n",
    "        h={} \n",
    "        p={} \n",
    "        b = 2 \n",
    "        lst = [] \n",
    "        tlst = []\n",
    "        vlst = []\n",
    "        for subdir, dirs, files in os.walk(os.path.join(self.args.data_root, self.args.tracked_avails)): \n",
    "            for file in files: \n",
    "                key, value = os.path.basename(subdir), file \n",
    "                h.setdefault(key, []).append(value) \n",
    "        #print(\"combination\")\n",
    "        for i in h.keys(): \n",
    "            c = list(itertools.combinations(h[i], b))\n",
    "            for t in c:\n",
    "                tlst.append(t)\n",
    "                vlst.append(i)\n",
    "        \n",
    "        p = {\n",
    "            'video_names': vlst,\n",
    "            'file_names' : tlst}         \n",
    "        #print('Tracked Avails - Combinations')\n",
    "        #print(\"combination\")\n",
    "        return (p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def creating_dataframe(self):\n",
    "        p = self.tracked_avails_list\n",
    "    \n",
    "# creating another list of video_names, object1, object2 for the dictionary:\n",
    "    \n",
    "    \n",
    "    \n",
    "        obj1 =[]\n",
    "        obj2 =[]\n",
    "    \n",
    "        for files in p['file_names']:\n",
    "            k = files\n",
    "            obj1.append(k[0].split('_')[2].split('.pkl')[0])\n",
    "            obj2.append(k[1].split('_')[2].split('.pkl')[0])\n",
    "\n",
    "\n",
    "        dict  = {\"video_names\": p['video_names'],\n",
    "                    \"obj1\": obj1,\n",
    "                    \"obj2\": obj2,\n",
    "                    \"values_names\": p['file_names']}\n",
    "\n",
    "        \n",
    "# Here all the object id and video names are stored in the dataframe \n",
    "\n",
    "\n",
    "        pickle_dataframe = pd.DataFrame(dict)\n",
    "        pickle_dataframe['obj1'] = pickle_dataframe['obj1'].astype('int32')\n",
    "        pickle_dataframe['obj2'] = pickle_dataframe['obj2'].astype('int32')\n",
    "        klst = []\n",
    "        flst = []\n",
    "        for i in range(0, len(pickle_dataframe)):\n",
    "            klst.append(self.return_common_frames(pickle_dataframe['values_names'][i][0], pickle_dataframe['values_names'][i][1], pickle_dataframe['video_names'][i], args.tracked_avails ,args.data_root))\n",
    "            flst.append(self.return_common_frames_accident_frame(pickle_dataframe['values_names'][i][0], pickle_dataframe['values_names'][i][1], pickle_dataframe['video_names'][i], args.tracked_avails ,args.data_root))\n",
    "\n",
    "        pickle_dataframe['common_frames'] = klst\n",
    "        pickle_dataframe['accident_frame_common_frames'] = flst\n",
    "        #print('creating df')\n",
    "        #return (self.annotation_file( os.path.join(r'E:\\DoTA_Avail_220726\\NEgo', \"DoTA_Non_Ego_Done_220216_relabelled.xlsx\"), pickle_dataframe))\n",
    "        pickle_dataframe=pickle_dataframe[pickle_dataframe['common_frames'].notna()]\n",
    "        return (pickle_dataframe)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# This fucntion will extract all the common frames_id if exists between the two object pickle files collected \n",
    "# from the tracked_avails\n",
    "# This function is called in the creating_dataframe function in line no.46\n",
    "# Returns a list of particular common frame id and stored in the pickle_dataframe['common_frames'] \n",
    "    def return_common_frames(self, filename_1, filename_2, video_name, folder_name, root_folder):\n",
    "    \n",
    "        lst = []\n",
    "#   file = pickle_dataframe['values_names'][0][0]\n",
    "#   part = os.path.join(args.data_root, args.tracked)\n",
    "#   file  = os.path.join(part, pickle_dataframe['video_names'][0], file)\n",
    "#   print(file)\n",
    "    \n",
    "        path_1 = os.path.join(root_folder, folder_name, video_name, filename_1)\n",
    "        path_2 = os.path.join(root_folder, folder_name, video_name, filename_2)\n",
    "    \n",
    "        with open(path_1,'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "        \n",
    "        with open(path_2, 'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "    \n",
    "#   print(\"x:\", x['frame_id'], \"y:\", y['frame_id'])\n",
    "        for item in set(x['frame_id']).intersection(y['frame_id']):\n",
    "            if item is not None:\n",
    "                lst.append(item)\n",
    "            else:\n",
    "                continue\n",
    "          \n",
    "    \n",
    "        if len(lst) < 1 :\n",
    "             return None \n",
    "        else :\n",
    "            return lst\n",
    "\n",
    "        \n",
    "        \n",
    "    def return_common_frames_accident_frame(self, filename_1, filename_2, video_name, folder_name, root_folder):\n",
    "    \n",
    "        \n",
    "        accident_frame_list=[]\n",
    "#   file = pickle_dataframe['values_names'][0][0]\n",
    "#   part = os.path.join(args.data_root, args.tracked)\n",
    "#   file  = os.path.join(part, pickle_dataframe['video_names'][0], file)\n",
    "#   print(file)\n",
    "    \n",
    "        path_1 = os.path.join(root_folder, folder_name, video_name, filename_1)\n",
    "        path_2 = os.path.join(root_folder, folder_name, video_name, filename_2)\n",
    "    \n",
    "        with open(path_1,'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "        \n",
    "        with open(path_2, 'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "    \n",
    "#   print(\"x:\", x['frame_id'], \"y:\", y['frame_id'])\n",
    "        \n",
    "        #set1 = list(int(x['accident_frame_id']))\n",
    "        #set2 = list(int(y['accident_frame_id']))\n",
    "        #print(set(set1).intersection(set2))\n",
    "        #print(x['accident_frame_id'], y['accident_frame_id'])\n",
    "        #for frame in set(x['accident_frame_id']).intersection(y['accident_frame_id']):\n",
    "        if x['accident_frame_id'] == y['accident_frame_id'] :\n",
    "            accident_frame_list.append(x['accident_frame_id'])\n",
    "\n",
    "            #if frame is not None:\n",
    "            #    accident_frame_list.append(frame)\n",
    "            #else:\n",
    "            #    continue        \n",
    "    \n",
    "        if len(accident_frame_list) < 1:\n",
    "             return None \n",
    "        else :\n",
    "            return accident_frame_list\n",
    "\n",
    "\n",
    "# This file is manually labelled and used as a filter for the extraction of accidental groups \n",
    "# Thus the original dataframe underwent left join with the excel annotation file \n",
    "# Left Join - When file X is left joined with File Y, it only shows all those values that are in X and in (X && Y)\n",
    "# if left join is done on X with respect to Y.\n",
    "# Therefore, all the values that are stored in excel file will be filtered with the pickle dataframe rest all the \n",
    "# drop.na() is implemented to deleted Nan Values there \n",
    "# At the end it will return pickle_dataframe_filtered\n",
    "\n",
    "    def annotation_file(self, csv_file_path, pickle_dataframe):\n",
    "    \n",
    "        excel_df = pd.read_excel(csv_file_path, sheet_name=1)\n",
    "        filter =  (excel_df['acc_obj_id1'] != 0) & (excel_df['acc_obj_id2'] != 0) \n",
    "        excel_df = excel_df.where(filter).dropna()\n",
    "\n",
    "        excel_df['acc_obj_id1'] = excel_df['acc_obj_id1'].astype(int)\n",
    "        excel_df['acc_obj_id2'] = excel_df['acc_obj_id2'].astype(int)\n",
    "    \n",
    "        pickle_dataframe_filtered = pd.merge(pickle_dataframe, excel_df, left_on=['video_names','obj1', 'obj2'], right_on = ['Name','acc_obj_id1', 'acc_obj_id2'], how='left')\n",
    "        pickle_dataframe_filtered = pickle_dataframe_filtered.dropna()\n",
    "\n",
    "        #print('pickle_dataframe')\n",
    "        return pickle_dataframe_filtered\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "# We need the common frame_ids, their location and need to extact the arrays-list of hidden states from that location \n",
    "# From here all the below written functions will be called in the __get_item__() because here we are collecting the \n",
    "# the list of all the hidden states generated in the system\n",
    "    def extraction(self, file_name_1, file_name_2,  common_frames, video_names):\n",
    "        #print('filename1:', file_name_1, 'filename2:', file_name_2)\n",
    "            \n",
    "#   Now we will create a path for the Hidden state \n",
    "\n",
    "        file_path_1 = os.path.join(self.args.data_root, self.args.hiddens, file_name_1)\n",
    "        file_path_2 = os.path.join(self.args.data_root, args.hiddens, file_name_2)\n",
    "\n",
    "#   File path generated \n",
    "\n",
    "# Now we will open the hidden state based pickle file that needs to be run \n",
    "        with open(file_path_1,'rb') as f:\n",
    "            hidden_x = pickle.load(f)\n",
    "            \n",
    "            \n",
    "        with open(file_path_2,'rb') as f:\n",
    "            hidden_y = pickle.load(f)\n",
    "#Target risk score:\n",
    "        if (hidden_x['target_risk_score'][0] > 0.0 and hidden_y['target_risk_score'][0] > 0.0):\n",
    "            target_risk_score = np.array([1.0])\n",
    "        else:\n",
    "            target_risk_score = np.array([0.0])\n",
    "        #if (hidden_x['target_risk_score'] > hidden_y['target_risk_score']):\n",
    "        #    target_risk_score = hidden_x['target_risk_score']\n",
    "        #else:\n",
    "        #    target_risk_score = hidden_y['target_risk_score']    \n",
    "        \n",
    "\n",
    "# Extracting the file path of tracked avails \n",
    "        file_path_tracked_avail_1 = os.path.join(self.args.data_root, self.args.tracked_avails, video_names,file_name_1)\n",
    "        file_path_tracked_avail_2 = os.path.join(self.args.data_root, self.args.tracked_avails, video_names, file_name_2)\n",
    "\n",
    "\n",
    "# First Tracked avail pickle file to be imported\n",
    "        with open(file_path_tracked_avail_1,'rb') as f:\n",
    "            file_path_tracked_avail_pickle_1 = pickle.load(f)\n",
    "\n",
    "\n",
    "# Second tracked avail pickle file to be imported         \n",
    "        with open(file_path_tracked_avail_2,'rb') as f:\n",
    "            file_path_tracked_avail_pickle_2 = pickle.load(f)\n",
    "        \n",
    "\n",
    "        file_1_lst = []\n",
    "        file_2_lst = []\n",
    "\n",
    "# Position of common frames are extracted from the tracked avails pickle files\n",
    "        for i in range(len(common_frames)):\n",
    "            for j in file_path_tracked_avail_pickle_1['frame_id']:\n",
    "                if common_frames[i] == j:\n",
    "                    file_1_lst.append(list(file_path_tracked_avail_pickle_1['frame_id']).index(j))\n",
    "\n",
    "        for i in range(len(common_frames)):\n",
    "            for j in file_path_tracked_avail_pickle_2['frame_id']:\n",
    "                if common_frames[i] == j:\n",
    "                    file_2_lst.append(list(file_path_tracked_avail_pickle_2['frame_id']).index(j))\n",
    "    \n",
    "        return (self.appending_hidden_states( hidden_x, hidden_y, common_frames, file_1_lst, file_2_lst), target_risk_score)\n",
    "    \n",
    "     \n",
    "\n",
    "# Appending the hidden states into the array and returning the appended array from the same \n",
    "# The attributes that have been taken into consideration are the x, y as the pickle files of hidden states,\n",
    "# common frames from the data_frame and list1 and list 2 collected from the extraction function is passed here\n",
    "# It returns the list of appended ndarray in the form of (x,10,2048) shape i.e., z \n",
    "\n",
    "    def appending_hidden_states(self, x, y, common_frames, lst_1, lst_2):\n",
    "        #print(x['hidden_state'][0],lst_1,lst_2)\n",
    "        #min_range = min(len(x['hidden_state']), len(y['hidden_state']))\n",
    "        min_range=min(len(lst_1), len(lst_2))\n",
    "        z = np.zeros([min_range, len(x['hidden_state'][0]), 2048], dtype = np.float64)\n",
    "\n",
    "        for k in range(min_range):\n",
    "            for i in range(0, len(x['hidden_state'][0])):\n",
    "                z[k][i] = list(np.append(x['hidden_state'][lst_1[k]][i], y['hidden_state'][lst_2[k]][i]))\n",
    "    \n",
    "    \n",
    "        return z    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae444bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = dataloader_split(args,args.tracked_avails,'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f338d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TRAINING ##########\n",
    "\n",
    "# %load training.py\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torchsummaryX import summary\n",
    "\n",
    "from loss import train_loss_cal, val_loss_cal, test_results\n",
    "from models import AP\n",
    "\n",
    "#from .utils import dataloaderHidden\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    torch.cuda.set_device(args.device)\n",
    "    args.dec_hidden_size = args.box_enc_size + args.dm_enc_size\n",
    "\n",
    "    print(\">> Setting the Accident Prediction model ... \")\n",
    "    AP_model = AP(args).to(args.device)\n",
    "    all_params = AP_model.parameters()\n",
    "    optimizer = optim.Adam(all_params, lr=args.lr)\n",
    "\n",
    "    dataloader_params ={\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"shuffle\": args.shuffle\n",
    "        }\n",
    "\n",
    "    val_set = dataloader_split(args, args.tracked_avails, 'Test')\n",
    "    #print(\">> Number of validation samples:\", val_set.__len__())\n",
    "    val_gen = data.DataLoader(val_set, **dataloader_params)\n",
    "\n",
    "        \n",
    "    print(\">> Check the Model's architecture\")\n",
    "    summary(AP_model,\n",
    "            # torch.zeros(1, args.segment_len, args.pred_timesteps, args.dec_hidden_size).to(device)\n",
    "            torch.zeros(1, args.segment_len, args.pred_timesteps,  args.dec_hidden_size).to(args.device)\n",
    "            )\n",
    "\n",
    "    # MODEL TRAINING\n",
    "    best_val_ap = None\n",
    "    best_val_hm = None\n",
    "    before_ATTC = 0.\n",
    "\n",
    "    inform = np.zeros((args.train_epoch, 6))\n",
    "\n",
    "    checkpoint_folder = os.path.join(args.checkpoint, args.trial_name)\n",
    "    if not os.path.exists(checkpoint_folder):\n",
    "        os.makedirs(checkpoint_folder)\n",
    "\n",
    "    bap_model_name = 'best_AP_model_AP.pt'\n",
    "    bap_full = os.path.join(checkpoint_folder, bap_model_name)\n",
    "\n",
    "    #bhm_model_name = 'best_AP_model_HM.pt'\n",
    "    bhm_model_name = args.best_harmonic_model\n",
    "    bhm_full = os.path.join(checkpoint_folder, bhm_model_name)\n",
    "\n",
    "    summary_folder = os.path.join(args.summary, args.trial_name)\n",
    "    if not os.path.exists(summary_folder):\n",
    "        os.makedirs(summary_folder)\n",
    "    print(\">> Train data root:\", os.path.join(args.data_root, args.train_root))\n",
    "    writer = SummaryWriter(summary_folder)\n",
    "\n",
    "    for epoch in range(1, args.train_epoch+1):\n",
    "        print(\"\\n\")\n",
    "        print(\"=====================================\")\n",
    "        print(\"// Epoch :\", epoch)\n",
    "        # regenerate the training dataset\n",
    "        train_set = dataloader_split(args, args.tracked_avails, 'Train')\n",
    "        train_gen = data.DataLoader(train_set, **dataloader_params)\n",
    "        #print(\" Number of training samples:\", train_set.__len__())\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        #===== train\n",
    "        train_loss, train_AP, train_ATTC = train_loss_cal(epoch, AP_model, optimizer, train_gen, before_ATTC, verbose=True)\n",
    "        train_hm = 2.0 * train_AP * train_ATTC / (train_AP + train_ATTC)\n",
    "        writer.add_scalar('data/train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('data/train_AP', train_AP, epoch)\n",
    "        writer.add_scalar('data/train_ATTC', train_ATTC, epoch)\n",
    "        writer.add_scalar('data/train_Harmonic', train_hm, epoch)\n",
    "        inform[epoch-1,0] = train_AP\n",
    "        inform[epoch-1,1] = train_ATTC\n",
    "        inform[epoch-1,2] = train_hm\n",
    "\n",
    "        #===== validation\n",
    "        val_loss, val_AP, val_ATTC = val_loss_cal(epoch, AP_model, val_gen, before_ATTC, verbose=True)\n",
    "        val_hm = 2.0 * val_AP * val_ATTC / (val_AP + val_ATTC)\n",
    "        writer.add_scalar('data/val_loss', val_loss, epoch)\n",
    "        writer.add_scalar('data/val_AP', val_AP, epoch)\n",
    "        writer.add_scalar('data/val_ATTC', val_ATTC, epoch)\n",
    "        writer.add_scalar('data/val_Harmonic', val_hm, epoch)\n",
    "        inform[epoch-1,3] = val_AP\n",
    "        inform[epoch-1,4] = val_ATTC\n",
    "        inform[epoch-1,5] = val_hm\n",
    "\n",
    "        before_ATTC = train_ATTC\n",
    "\n",
    "\n",
    "        # print time\n",
    "        elipse = time.time() - start\n",
    "        print(\"Elipse: \", elipse)\n",
    "\n",
    "\n",
    "        if best_val_ap is None:\n",
    "            best_val_ap = val_AP\n",
    "            torch.save(AP_model.state_dict(), bap_full)\n",
    "        elif best_val_ap < val_AP:\n",
    "            best_val_ap = val_AP\n",
    "            torch.save(AP_model.state_dict(), bap_full)\n",
    "\n",
    "        if best_val_hm is None:\n",
    "            best_val_hm = val_hm\n",
    "            torch.save(AP_model.state_dict(), bhm_full)\n",
    "        elif best_val_hm < val_hm:\n",
    "            best_val_hm = val_hm\n",
    "            torch.save(AP_model.state_dict(), bhm_full)\n",
    "\n",
    "        # save checkpoint per epoch\n",
    "        save_name = 'epoch_{:03d}'.format(epoch) + '.pt'\n",
    "        full = os.path.join(checkpoint_folder, save_name)\n",
    "        torch.save(AP_model.state_dict(), full)\n",
    "\n",
    "    df = pd.DataFrame(inform)\n",
    "    df.to_csv(os.path.join(checkpoint_folder, 'train_val_inform.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0131bd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Setting the Accident Prediction model ... \n",
      ">> Check the Model's architecture\n",
      "==============================================================================\n",
      "                               Kernel Shape  Output Shape    Params Mult-Adds\n",
      "Layer                                                                        \n",
      "0_avgpool                                 -  [1, 2048, 1]         -         -\n",
      "1_fc_layer                      [2048, 128]      [1, 128]  262.272k  262.144k\n",
      "2_feature_to_output.Linear_0       [128, 1]        [1, 1]     129.0     128.0\n",
      "3_feature_to_output.Sigmoid_1             -        [1, 1]         -         -\n",
      "4_avgpool                                 -  [1, 2048, 1]         -         -\n",
      "5_fc_layer                      [2048, 128]      [1, 128]         -  262.144k\n",
      "6_feature_to_output.Linear_0       [128, 1]        [1, 1]         -     128.0\n",
      "7_feature_to_output.Sigmoid_1             -        [1, 1]         -         -\n",
      "8_avgpool                                 -  [1, 2048, 1]         -         -\n",
      "9_fc_layer                      [2048, 128]      [1, 128]         -  262.144k\n",
      "10_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "11_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "12_avgpool                                -  [1, 2048, 1]         -         -\n",
      "13_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "14_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "15_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "16_avgpool                                -  [1, 2048, 1]         -         -\n",
      "17_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "18_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "19_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "20_avgpool                                -  [1, 2048, 1]         -         -\n",
      "21_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "22_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "23_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "24_avgpool                                -  [1, 2048, 1]         -         -\n",
      "25_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "26_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "27_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "28_avgpool                                -  [1, 2048, 1]         -         -\n",
      "29_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "30_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "31_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "32_avgpool                                -  [1, 2048, 1]         -         -\n",
      "33_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "34_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "35_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "36_avgpool                                -  [1, 2048, 1]         -         -\n",
      "37_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "38_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "39_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "40_avgpool                                -  [1, 2048, 1]         -         -\n",
      "41_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "42_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "43_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "44_avgpool                                -  [1, 2048, 1]         -         -\n",
      "45_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "46_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "47_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "48_avgpool                                -  [1, 2048, 1]         -         -\n",
      "49_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "50_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "51_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "52_avgpool                                -  [1, 2048, 1]         -         -\n",
      "53_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "54_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "55_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "56_avgpool                                -  [1, 2048, 1]         -         -\n",
      "57_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "58_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "59_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "60_avgpool                                -  [1, 2048, 1]         -         -\n",
      "61_fc_layer                     [2048, 128]      [1, 128]         -  262.144k\n",
      "62_feature_to_output.Linear_0      [128, 1]        [1, 1]         -     128.0\n",
      "63_feature_to_output.Sigmoid_1            -        [1, 1]         -         -\n",
      "------------------------------------------------------------------------------\n",
      "                         Totals\n",
      "Total params           262.401k\n",
      "Trainable params       262.401k\n",
      "Non-trainable params        0.0\n",
      "Mult-Adds             4.196352M\n",
      "==============================================================================\n",
      ">> Train data root: C:/DoTA_Avail_220726/NEgo/train\n",
      "\n",
      "\n",
      "=====================================\n",
      "// Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\heebah_non_ego\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sum = df.sum()\n",
      "C:\\Users\\user\\.conda\\envs\\heebah_non_ego\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive df 430\n",
      "negative df 432\n",
      "Before 5 frames 862\n",
      "After 5 frames 704\n",
      " Train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:13<00:00, 53.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- AP on Train dataset : 0.6679485094066766\n",
      "-- ATTC on Train dataset : 1.4457836180975883\n",
      " Validation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9039/9039 [00:39<00:00, 226.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- AP on Validation dataset : 0.17090597798460652\n",
      "-- ATTC on Validation dataset : 1.244542359740286\n",
      "Elipse:  415.6245415210724\n",
      "\n",
      "\n",
      "=====================================\n",
      "// Epoch : 2\n",
      "positive df 433\n",
      "negative df 432\n",
      "Before 5 frames 865\n",
      "After 5 frames 695\n",
      " Train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [00:13<00:00, 53.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- AP on Train dataset : 0.7484837022492472\n",
      "-- ATTC on Train dataset : 1.265192908490413\n",
      " Validation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9039/9039 [00:41<00:00, 218.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- AP on Validation dataset : 0.1798742985252285\n",
      "-- ATTC on Validation dataset : 1.2268127324906004\n",
      "Elipse:  451.8278546333313\n",
      "\n",
      "\n",
      "=====================================\n",
      "// Epoch : 3\n",
      "positive df 429\n",
      "negative df 432\n",
      "Before 5 frames 861\n",
      "After 5 frames 698\n",
      " Train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [00:12<00:00, 54.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- AP on Train dataset : 0.8057358010100739\n",
      "-- ATTC on Train dataset : 1.2540546950815832\n",
      " Validation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9039/9039 [00:41<00:00, 215.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23516\\2146756250.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23516\\850976848.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m#===== validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_AP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ATTC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_loss_cal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAP_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbefore_ATTC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mval_hm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mval_AP\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mval_ATTC\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_AP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_ATTC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\scene_agnostic_anticipation\\scene_agnostic_anticipation-main\\lib\\loss.py\u001b[0m in \u001b[0;36mval_loss_cal\u001b[1;34m(epoch, ap_model, val_gen, before_ATTC, verbose)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;31m# print(\"length of dict-all_pred:\", len(all_pred))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[1;31m# print(\"all_labels:\", all_labels.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[0mAP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mATTC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcal_ATTC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-- AP on Validation dataset :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-- ATTC on Validation dataset :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mATTC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\scene_agnostic_anticipation\\scene_agnostic_anticipation-main\\lib\\loss.py\u001b[0m in \u001b[0;36mcal_ATTC\u001b[1;34m(all_pred, all_labels, max)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m#Heebah --End--\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[0mRecall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2260\u001b[1;33m                           initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc46626",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### TESTING FARHAN - HEEBAH ###############\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torchsummaryX import summary\n",
    "\n",
    "from loss import train_loss_cal, val_loss_cal, test_results\n",
    "from models import AP\n",
    "\n",
    "#from .utils import dataloaderHidden\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import pandas as pd\n",
    "\n",
    "dataloader_params ={\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"shuffle\": args.shuffle\n",
    "        }\n",
    "\n",
    "val_set = dataloader_split(args, args.tracked_avails, 'Test')\n",
    "#print(\">> Number of validation samples:\", val_set.__len__())\n",
    "val_gen = data.DataLoader(val_set, **dataloader_params)\n",
    "\n",
    "AP_model = AP(args).to(args.device)\n",
    "AP_model.load_state_dict(torch.load(r'C:\\Users\\user\\Downloads\\scene_agnostic_anticipation\\scene_agnostic_anticipation-main\\lib\\weights\\best_AP_model_AP.pt'))\n",
    "\n",
    "val_loss, val_AP, val_ATTC = test_results(AP_model, val_gen, before_ATTC=0, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25b316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1334308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
